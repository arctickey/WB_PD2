---
title: "Prezentacja metodologii"
author: "Jan Borowski, Filip Chrzuszcz, Piotr Fic"
date: "14 05 2020"
output: ioslides_presentation
widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Wykorzystane zbiory danych  

```{r echo=FALSE, message=FALSE, warning=FALSE,cache=TRUE}
#path_to_datasets <- "/home/jan/Pulpit/WB/2020L-WarsztatyBadawcze-Imputacja/datasets/"
path_to_datasets <- "/home/piotr/Programowanie/WB/fork_grupy/2020L-WarsztatyBadawcze-Imputacja/datasets"
#path_to_datasets <- "/home/arctickey/2020L-WarsztatyBadawcze-Imputacja/datasets"

folder <- list.dirs(path_to_datasets)
folder <- folder[-1]
script_paths <- paste(folder, '/', 'code.R', sep='')

zbiory_danych <- as.data.frame(matrix(nrow = 14,ncol = 5))
colnames(zbiory_danych) <- c('Liczba Obserwacji','Liczba Kolumn','Procent braków danych','Balans Klas','Openml_ID')
iterator <- 1 
for(i in script_paths){
  source(i, chdir=T)
  zbiory_danych[iterator,2] <- paste0(length(colnames(dataset))-1,' ')
  zbiory_danych[iterator,1] <- paste0(length(dataset[,1]),' ')
  more_frequent  <- table(dataset[,target_column])[1]
  less_frequent <- table(dataset[,target_column])[2]
  suma <- more_frequent+less_frequent
  balans <- paste0(round(more_frequent/suma*100,0),'%/',round(less_frequent/suma*100,0),'%')
  zbiory_danych[iterator,4] <- balans
  number_of_missing <- sum(sapply(dataset, function(x) sum(is.na(x))))
  
  zbiory_danych[iterator,3] <- paste0(round(100*number_of_missing/(length(dataset[,1])*(length(colnames(dataset))-1)),2),'%')
  zbiory_danych[iterator,5] <- openml_id
  iterator <- iterator+1
}

knitr::kable(zbiory_danych, format = "html", table.attr = "style='height:40%;'")
```
## Podział zbirou na testowy i treningowy 
Każdy zbiór był wstępnie czyszczony a następnie dzielony w sposób losowy na podzbiór treningowy i testowy: 

- Zbiór treningowy **80%** obserwacji 
- Zbiór testowy **20%** obserwacji

Podział pozostawał taki sam dla wszystkich metod imputacji i użytych modeli.

## Wykorzystane techniki imputacji braków danych 
Przed przystąpieniem do imputacji usunęliśmy zmienną celu:

- Imputacja za pomocą **mediany** (14/14)
- Imputacja metodą z pakietu **mice** (13/14)
- Imputacja **missForest** (?/14)
- Imputacja **missMDA** (11/14)
- Imputacja **softImpute + mediana** (12/14)
- Imputacja funkcją **irmi** z pakietu **VIM** (?????) Jescze nie przeprowadzona 

Po imputacji do zbioru ponownie dołączono kolumnę celu.

## Przebieg imputacji

<center>
![](diagram_imput.png)


## Kodowanie zmiennych kategorycznych

Uzupełnione zbiory danych zawierają wiele zmiennych kategorycznych.  
Podejścia odrzucone:

- one-hot encoding
- ordinal encoding

Wybrane rozwiązanie:

- **target encoding**

Metoda kodowania jest uniwersalna i powszechnie używana.  
Dodatkowo usunięto w kilku zbiorach zmienne, które zawierały
bardzo dużo unikalnych wartości.

## Kodowanie zmiennych kategorycznych

1. Zbiór podzielony na część treningową i testową
2. Wyliczenie parametrów **target encoding** na zbiorze treningowym
3. Kodowanie zmiennych w obu zbiorach

## Narzędzie do kodowania

Pakiet [H<sub>2</sub>O](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html) napisany w języku Java, udostępniający API dla R i python. Wykorzystuje obliczenia wielowątkowe.  
Kompletne narzędzie do uczenia maszynowego.  
Użyty moduł do target encoding:  

- dodanie szumu 
- średnie ważone względem liczności grup


## Wybrane algorytmy

Zbiory danych są gotowe do użycia przez modele uczenia maszynowego.  
Wykorzystane modele (*mlr name*):

- **regresja logistyczna**: *classif.glmnet*
- **las losowy**: *classif.ranger*
- **SVM**: *classif.svm*
- **XGBoost**: *classif.xgboost*

## Ewaluacja modeli

<center>
![](schemat.png)
</center>

## Ewaluacja modeli

1. Podział zbiorów na część treningową i testową  
jednakowy dla wszystkich modeli i metod imputacji
2. Podczas kroswalidacji strojenie parametrów modeli:

- **regresja logistyczna**: *alpha, nlambda*
- **las losowy**: *num.trees, min.node.size, mtry*
- **SVM**: *gamma*
- **XGBoost**: *eta, gamma, max_depth, subsample*

## Metryki ewaluacji

Jako podstawowy odnośnik  
 
$Accuracy = \frac{TP+TN}{TP+FP+FN+TN}$

Uwzględniając niezbalansowanie części zbiorów

$F1 = \frac{2\cdot precision \cdot recall}{precision+recall}$

Gdzie:

- precision $\frac{TP}{TP+FP}$
- recall $\frac{TP}{TP+FN}$


## Wyniki
Jak opisaliśmy wcześniej zdecydowaliśmy się na użycie dwóch miar skuteczności algorytmów: 

- F1 score
- Accuracy


Te dwie miary dobrze się uzupełniają, gdyż pozwalają dobrze zmierzyć skuteczności naszych imputacji oraz algorytmów zarówno na zbiorach zbalansowanych, jak i niezbalansowanych.

## Rozkład miary F1

```{r echo=FALSE, message=FALSE,warning=FALSE}
library(ggplot2)
library(tidyverse)
data <- read_csv('./wyniki_csv/RESULT.csv')
p<-ggplot(data, aes(x=method, y=F1, color=method)) +
  geom_boxplot()+theme_minimal()+labs(x="",y="")+ggtitle("Wyniki średnie F1")
p
```

## Odchylenie standardowe F1

```{r echo=FALSE, message=FALSE,warning=FALSE}
knitr::kable(data %>% group_by(method) %>% summarise(F1_std=sd(F1,na.rm=TRUE)))
```

## Rozkład miary Accuracy

```{r echo=FALSE, message=FALSE,warning=FALSE}
p1<-ggplot(data, aes(x=method, y=acc, color=method)) +
  geom_boxplot()+theme_minimal()+labs(x="",y="")+ggtitle("Wyniki średnie accuracy")
p1
```

## Odchylenie standardowe Accuracy

```{r echo=FALSE, message=FALSE,warning=FALSE}
knitr::kable(data %>% group_by(method) %>% summarise(acc_std=sd(acc,na.rm=TRUE)))
```

## Pierwsze wnioski

Na podstawie tych wykresów można wnioskować o ogólnych wynikach, jednkaże nie można się nimi sugerować zbyt bardzo, gdyż wyniki uzyskane przez algorytmy rożnią się w zależności od trudności zbioru a także od użytego algorytmu uczenia maszynowego. Sprawdźmy więc jak wyniki F1 różnią się w zależności od rodzaju imputacji oraz użytego algorytmu.


##

```{r echo=FALSE, message=FALSE,warning=FALSE}
data1 <- data %>% group_by(method,algorithm) %>% summarise(F1_mean=mean(F1,na.rm=TRUE))
p2<-ggplot(data1, aes(x=method, y=F1_mean, color=algorithm)) +
  geom_boxplot()+theme_minimal()+labs(x="",y="")+ggtitle("Wyniki F1 pośród róznych algorytmów oraz imputacji")
p2
```


## Ranking metod

Jako bardziej wiarygodną próbę oceny skuteczności imputacji spróbujmy uszeregować je wedle następującego schematu. Weźmy wynik każdej z metod imputacji na każdym ze zbiorów. Wynik taki definiujemy jako średnia ze wszystkich algorytmów uczenia maszynowego puszczonych na danym zbiorze przy danej imputacji. Następnie sortujemy wynik od najwyższego i przyznajemy algorytmom punkty za każde miejsce. Końcowo punkty sumujemy a ta metoda imputacji, która punktów uzyska najmniej wygrwa nasz ranking.


##

```{r echo=FALSE, message=FALSE,warning=FALSE}

library(R.utils)
library(prob)
dane_o_imputacij <- data
ids <- dane_o_imputacij$dataset
x <- c(1,2,3)
algosy <- c('log_reg','xgb','rf','svm')

for (i in ids) {
  
  indeksy <- ifelse(dane_o_imputacij$dataset==i,TRUE,FALSE)
  dane_id <-  dane_o_imputacij[indeksy,]
  expectet <- c('median','softimpute','missmda')
  founded <- dane_id$method
  insert_<- setdiff(expectet,founded)
  
  for (j in insert_){
    for(a in algosy){
  dane_o_imputacij <- rbind(dane_o_imputacij,c(1,i,j,a,0,0))
  
  }
  dane_wynik <- dane_o_imputacij
  }
}
data<- dane_wynik
data$F1 <- as.double(data$F1)
data$acc <- as.double(data$acc)

data[is.na(data)] <- 0
data3 <- data %>% group_by(dataset,method) %>% summarise(mean=mean(acc,na.rm=TRUE))
order_scores <- data3 %>% group_by(dataset) %>% mutate(good_ranks = rank(-mean, ties.method = "max"))
order_scores1 <- order_scores %>% group_by(method) %>% summarise(wynik = sum(good_ranks))
order_scores1 <- order_scores1[order(order_scores1$wynik),]

knitr::kable(order_scores1)
```

Na podstawie tych wyników można wyciągnąc już dużo głębsze wnioski, dzięki możliwości rzetelnego porównania sprawowania się algorytmów imptuacji.

## Rankig imputacja + algorytm

Na podobnej zasadzie jak przed chwilą zdecydowaliśmy się poszukać najlepszej pary algorytm + imputacja


```{r echo=FALSE, message=FALSE,warning=FALSE}
data4 <- data %>% group_by(dataset,method,algorithm) %>% summarise(mean=mean(acc,na.rm=TRUE))
order_scores2 <- data4 %>% group_by(dataset) %>% mutate(good_ranks = rank(-mean, ties.method = "max"))
order_scores3 <- order_scores2 %>% group_by(method,algorithm) %>% summarise(wynik = sum(good_ranks))
order_scores3 <- order_scores3[order(order_scores3$wynik),]
knitr::kable(order_scores3)

```

## Kolejna forma rankingu

Jako drugą formę uszeregowania algorytmów imputacji zdecydowaliśmy się użyć następującej formuły. Traktujemy medianę jako podstawową formę imputacji i to do niej będziemy przyrównywać wszystkie pozostałe metody. Chcemy sprawdzić o ile średnio predykcje wykonane przez inne algorytmy różniły się od mediany. Jako jedną predykcję rozumiemy średnią ze wszystkich metod imputacji.

```{r echo=FALSE, message=FALSE,warning=FALSE}
data5 <- data %>% group_by(dataset,method) %>% summarise(mean=mean(acc,na.rm=TRUE))
un <- length(unique(data5$dataset))
data5[data5$method=='median','method'] = 'aaamedian'
order_scores2 <- data5 %>% group_by(dataset) %>% mutate(diff = max(mean) - mean)
order_scores2 <- data5 %>% group_by(dataset) %>% mutate(diff = mean - dplyr::lag(mean, default = mean[1]))
scores <- order_scores2 %>% group_by(method) %>% summarise(score = sum(diff)/un)
x <- scores[scores$method!='aaamedian',]
knitr::kable(x[order(x$score,decreasing = TRUE),])

```

## Czasy

Kolejną płaszczyzną na której można porównywać algorytmy inputacji danych jest czas ich działania. Oczywistym jest, iż nawet najlepiej działający algorytm będzie nie do zaakceptowania, jeśli będzie działał zbyt długo.

##

```{r echo=FALSE, message=FALSE,warning=FALSE}

czas_mice <- read_csv('./imputed_data/czasy/mice.csv')
czas_mice['algo'] <- 'mice'
czas_median <-  read_csv('./imputed_data/czasy/median.csv')
czas_median['algo'] <- 'median'
czas_missmda <-  read_csv('./imputed_data/czasy/missmda.csv')
czas_missmda['algo'] <- 'missmda'
czas_sofimpute <-  read_csv('./imputed_data/czasy/softimpute.csv')
czas_sofimpute['algo'] <- 'softimpute'

data <- rbind(czas_mice,czas_median,czas_missmda,czas_sofimpute)

p1<-ggplot(data, aes(x=algo, y=log1p(V2), color=algo)) +
  geom_boxplot()+theme_minimal()+labs(x="",y="")+ggtitle("Czasy imputacji")
p1

```

## Podsumowanie

