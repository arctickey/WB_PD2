---
title: "Prezentacja metodologii"
author: "Jan Borowski, Filip Chrzuszcz, Piotr Fic"
date: "14 05 2020"
output: ioslides_presentation
widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Wykorzystane zbiory danych  

```{r echo=FALSE, message=FALSE, warning=FALSE,cache=TRUE}
#path_to_datasets <- "/home/jan/Pulpit/WB/2020L-WarsztatyBadawcze-Imputacja/datasets/"
path_to_datasets <- "/home/piotr/Programowanie/WB/fork_grupy/2020L-WarsztatyBadawcze-Imputacja/datasets"
#path_to_datasets <- "/home/arctickey/2020L-WarsztatyBadawcze-Imputacja/datasets"

folder <- list.dirs(path_to_datasets)
folder <- folder[-1]
script_paths <- paste(folder, '/', 'code.R', sep='')

zbiory_danych <- as.data.frame(matrix(nrow = 14,ncol = 5))
colnames(zbiory_danych) <- c('Ilość Obserwacji','Ilość Kolumn','Procent braków danych','Balans Klas','Openml_ID')
iterator <- 1 
for(i in script_paths){
  source(i, chdir=T)
  zbiory_danych[iterator,2] <- paste0(length(colnames(dataset))-1,' ')
  zbiory_danych[iterator,1] <- paste0(length(dataset[,1]),' ')
  more_frequent  <- table(dataset[,target_column])[1]
  less_frequent <- table(dataset[,target_column])[2]
  suma <- more_frequent+less_frequent
  balans <- paste0(round(more_frequent/suma*100,2),'%/',round(less_frequent/suma*100,2),'%')
  zbiory_danych[iterator,4] <- balans
  number_of_missing <- sum(sapply(dataset, function(x) sum(is.na(x))))
  
  zbiory_danych[iterator,3] <- paste0(round(100*number_of_missing/(length(dataset[,1])*(length(colnames(dataset))-1)),2),'%')
  zbiory_danych[iterator,5] <- openml_id
  iterator <- iterator+1
}

knitr::kable(zbiory_danych, format = "html", table.attr = "style='height:40%;'")
```
## Podział zbirou na testowy i treningowy 
Każdy zbiór był wstępnie czyszczony a następnie dzielony w sposób losowy na podzbiór treningowy i testowy: 

- Zbiór treningowy **80%** obserwacji 
- Zbiór testowy **20%** obserwacji

Podział pozostawał taki sam dla wszystkich metod imputacji i użytych modeli.

## Wykorzystane techniki imputacji braków danych 
Przed przystąpieniem do imputacji usunęliśmy zmienną celu:

- Imputacja za pomocą **mediany** (14/14)
- Imputacja metodą z pakietu **mice** (13/14)
- Imputacja **missForest** (?/14)
- Imputacja **missMDA** (11/14)
- Imputacja **softImpute + mediana** (12/14)
- Imputacja funkcją **irmi** z pakietu **VIM** (?????) Jescze nie przeprowadzona 

Po imputacji do zbioru ponownie dołączono kolumnę celu.

## Przebieg imputacji

<center>
![](diagram_imput.png)


## Kodowanie zmiennych kategorycznych

Uzupełnione zbiory danych zawierają wiele zmiennych kategorycznych.  
Podejścia odrzucone:

- one-hot encoding
- ordinal encoding

Wybrane rozwiązanie:

- **target encoding**

Metoda kodowania jest uniwersalna i powszechnie używana.  
Dodatkowo usunięto w kilku zbiorach zmienne, które zawierały
bardzo dużo unikalnych wartości.

## Kodowanie zmiennych kategorycznych

1. Zbiór podzielony na część treningową i testową
2. Wyliczenie parametrów **target encoding** na zbiorze treningowym
3. Kodowanie zmiennych w obu zbiorach

## Narzędzie do kodowania

Pakiet [H<sub>2</sub>O](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html) napisany w języku Java, udostępniający API dla R i python. Wykorzystuje obliczenia wielowątkowe.  
Kompletne narzędzie do uczenia maszynowego.  
Użyty moduł do target encoding:  

- dodanie szumu 
- średnie ważone względem liczności grup


## Wybrane algorytmy

Zbiory danych są gotowe do użycia przez modele uczenia maszynowego.  
Wykorzystane modele (*mlr name*):

- **regresja logistyczna**: *classif.glmnet*
- **las losowy**: *classif.ranger*
- **SVM**: *classif.svm*
- **XGBoost**: *classif.xgboost*

## Ewaluacja modeli

<center>
![](schemat.png)
</center>

## Ewaluacja modeli

1. Podział zbiorów na część treningową i testową  
jednakowy dla wszystkich modeli i metod imputacji
2. Podczas kroswalidacji strojenie parametrów modeli:

- **regresja logistyczna**: *alpha, nlambda*
- **las losowy**: *num.trees, min.node.size, mtry*
- **SVM**: *gamma*
- **XGBoost**: *eta, gamma, max_depth, subsample*

## Metryki ewaluacji

Jako podstawowy odnośnik  
 
$Accuracy = \frac{TP+TN}{TP+FP+FN+TN}$

Uwzględniając niezbalansowanie części zbiorów

$F1 = \frac{2\cdot precision \cdot recall}{precision+recall}$

Gdzie:

- precision $\frac{TP}{TP+FP}$
- recall $\frac{TP}{TP+FN}$

