---
output:
  pdf_document: default
bibliography: ["cytowania.bib"]
---

## Various data imputation techniques in R

*Authors: Jan Borowski, Filip Chrzuszcz, Piotr Fic  (Warsaw University of Technology)*

### Abstract

There are many suggestions how to deal with missing values in data sets problem. Some solutions are offered in publicly available packages for the R language. In our study, we tried to compare the quality of different methods of data imputation and their impact on the performance of machine learning models. We scored different algorithms on various data sets imputed by chosen packages. Results summary presents packages which enabled to achieve the best models predictions metrics. Moreover, duration of imputation was measured. 

### Introduction and Motivation

#### Background and related work

Missing observations in data sets is a common and difficult problem. In the field of machine learning, one of the key objects is the data set. Real-world data are often incomplete, which prevents the usage of many algorithms. Most implementations of machine learning models, available in popular packages, are not prepared to deal with missing values. Before creating a machine learning model, it is essential to solve the problem of missing observations. This requires user pre-processing of data. Some researches examined similarity between original and imputed data, in terms of descriptive statistics [@2_4_musil2002impcomp]. Missing data are common in medical sciences and impact of different imputation methods on analysis was measured [@2_4_bono2007medical]. Some studies show that imputation can improve the results of machine learning models and that more advanced techniques of imputation outperform basic solutions [@2_4_gustavo2003impclasif] [@2_4_su2008impclasif].    

#### Motivation

Various imputation techniques are implemented in different packages for the R language. Their performance is often analyzed independently and only in terms of imputation alone. Because of variety of available tools, it becomes uncertain which one package and method to use, when complete data set is needed for machine learning model. In our study we would like to examine, how methods offered by some popular packages perform on various data sets. We want to consider flexibility of these packages to deal with different data sets. The most important issue for us is the impact of performed imputation on later machine learning model performance. We are going to consider one specific type of machine learning tasks: *supervised binary classification*. Our aim is a comparison of metrics scores achieved by various models depending on chosen imputation method. 

#### Definition of missing data

At the beginning, clarifying the definition of missing data is necessary. Missing data means, that one or more variables have no data values in observations. This can be caused by various reasons, which we can formally define as follows, referring to @2_4_rubin1976mar:

- MCAR (Missing completely at random)\
Values are missing completely at random if the events that lead to lack of value are independent both of observable variable and of unobservable parameters. The missing data are simply a random subset of the data. Analysis performed on MCAR data is unbiased. However, data are rarely MCAR.
- MAR (Missing at random)\
Missingness of the values can be fully explained by complete variables. In other words, missing data are not affected by their characteristic, but are related to some or all of observed data. This is the most common assumption about missing data.
- MNAR (Missing not at random)\
When data are missing not at random, the missingness is related to the characteristic of variable itself.

#### Techniques of dealing with missing data

In case of preparing a data set for machine learning models we can generally distinguish two approaches. The first method is **omission**. From the data set we can remove observations with at least one missing value or we can remove whole variables where missing values are present. This strategy is appropriate if the features are MCAR. However, it is frequently used also when this assumption is not met. It is also useless when the percentage of missing values is high. The second approach is **imputation**, where values are filled in the place of missing data. There are many methods of imputation, which we can divide into two groups. **Single imputation** techniques use information of one variable with missing values. Popular method is filling missings with mean, median or mode of no missing values. More advanced are predictions from regression models which are applied on the mean and covariance matrix estimated by analysis of complete cases. The main disadvantage of single imputation is treating the imputed value as true value. This method does not take into account the uncertainty of the missing value prediction. For this reason **multiple imputation** was proposed. This method imputes *k* values, which leads to creating *k* complete data sets. The analysis or model is applied on each complete data set and finally results are consolidated. This approach keeps the uncertainty about the range of values which the true value could have taken. Additionally, multiple imputation can be used in both cases of MCAR and MAR data.

### Methodology
Experiment like this one can be performed involving many techniques we decide to divide our tests into 4 steps:

- Data Preparation,
- Data Imputation,
- Model Training,
- Model Evaluation.

Bellow we will explain every step in detail.

#### Data Preperation 
For test purposes we used 14 data sets form OpenML library [@2_4_R-OpenML]. Every data set is designed for binary classification and most of them contain numerical and categorical features. Most of the sets have a similar number of observations in both classes but some of them were very unbalanced. Before data imputation data set was prepared specific preparation are different for each data set but we commonly do:

- Removing features which didn't contain useful information (for example all observation have the same value)
- Correcting typos and converting all string to lower case to reduce the number of categories 
- Converting date to more than one column (for example "2018-03-31" can be converted to three column year, month and day)
- Removing or converting columns with too many categories

After cleaning data sets were transferred to the next step. 

#### Data Imputation
Clean data sets were split into two data sets train and test in proportion $1/4$ respectively. This split was performed randomly and only once for every data set that's mean every imputation technique used the same split. Imputation was performed separately for train and test sets. Before split we also remove the target column to avoid using it in imputation. For our study, we decided to choose five packages designed for missing data imputation in the R language and one self-implemented basic technique:

- **Mode and median**: Simple technique of filling missing values with mode (for categorical variables) and median (for continuous variables) of complete values in a variable. Implemented with basic R language functions.
- **mice**[@2_4_mice2011]: Package allows to perform multivariate imputation by chained equations (MICE), which is a type of multiple imputation. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by separate model.
- **missMDA**[@2_4_missMDA2016]: Package for multiple missing values imputation. Data sets are imputed with the principal component method, regularized iterative FAMD algorithm (factorial analysis for mixed data). First estimation of the number of dimensions for factorial analysis is essential.
- **missFOREST**[@2_4_missForest2012]: Package can be used for imputation with predictions of random forest model, trained on complete observations. Package works on data with complex interactions and non-linear relations. Enables parallel calculations.
- **softImpute**[@2_4_R-softImpute]: Package for matrix imputation with nuclear-norm regularization.  Algorithm works like EM, solving an optimization problem using a soft-thresholded SVD algorithm.  Works only with continuous variables.
- **VIM**[@2_4_VIM2016]: Package for visualization and imputation of missing values. It offers iterative robust model-based imputation (IRMI). In each iteration, one variable is used as a response variable and the remaining variables as the regressors.


First we use mode/median imputation, which is a very simple method and it is used as a base result for more complex algorithms to compare.
Imputation method form mice package don't require any form of help because can impute numeric and categorical features.
SoftImpute package works only with numeric features. To compare it with other algorithms on the same data we use SoftImput for numeric variables and mode for categorical variables. Alternatively, it is possible to use SoftImpute for numeric features and different algorithms for categorical variables, but we decided that this approach may lead to unreliable results.
MissForest algorithm can be used on both numeric and categorical features and is capable of performing imputation without any help of other methods.
Imputation method from Mice package also can be run on all types of data.
Iterative Robust Model-Based Imputation method from VIM package can impute all types of data. This method additionally creates new columns with information whether observation was imputed or not. We decided to do not use these columns, because other methods do not create them.
Last method which we covered is missMDA which also can be used to input numeric and categorical features.
After imputation we add back target variable to both sets. All methods work on the same parameters for all data sets. In case when for some reason method can't input some data set it was treated like "worst result" more information about it can be found in section 4.Model evaluation.

#### Model traing 
For classification task we use four classification algorithms:

- Extreme Gradient Boosting, 
- Random Forest, 
- Support Vector Machines,
- Linear Regression 

All methods were implemented in **mlr** package [@2_4_mlr] for hyperparameters tuning, we also used methods from the same package. For all data sets four classifiers were trained and tuned on the same train sets. To select parameters we used Grid Search. We will not focus on this part of the experiment. The most important part in this step, is that every model training was proceeded the same way. This mean that differences in results can be caused only by influence of previously used imputation technique. 

#### Model evaluation 
After previous steps, we have got trained models and test sets. In the final step we evaluate model and imputation. For every imputation and algorithm we calculate F1 score expressed by formula $2\frac{(precision)\cdot(recall)}{precision + recall}$ and accuracy. In case when imputation algorithm fail to impute some data set results for this set are thread as "the worst result". It means if u try to create a ranking it is always last (if more then one imputation fail all of them is placed last). A detailed discussion about results in the next section.

### Results

After the long and tedious process of data imputation it is finally time to evaluate our methods of imputation. Methods were trained and tested on 11 datasets and evaluated strictly using 2 methods mentioned earlier. Before score analysis it is worth mentioning that all algorithms were tested with optimal parameters, so score should be quite meaningful. Table below presents average metrics achieved by model, depending on imputation method. As described earlier, we decided to use two measures of algorithm effectiveness:

- F1 score
- Accuracy

These two measures complement each other well because they allow us to measure well the effectiveness of our imputations and algorithms on both balanced and unbalanced sets.
```{r echo=FALSE, message=FALSE, warning=FALSE , results='asis'}
library(knitr)
library(tidyverse)
scores <- readr::read_csv('./final_score/ostatecznev2.csv')
ramka <- scores %>% group_by(Method) %>% summarise(Accuracy = round(mean(Acc),2),F1 = round(mean(F1),2))
knitr::kable(ramka,caption='Average scores of model on imputation methods')
```

Our experiment definately did not find out the best imputation algoriithm, but we can derive some interesting conclusions from it.
First of all, we are going to treat median imputation as kind of a baseline for our algorithms. Suprisngly it seems to perform quite well, amongst the others, often quite sophisticated methods. It achieved over 80% of accuracy on average. However, it is hard to decide whether it is a high score or not, because we are only able to compare algorithms between the others. To say anything more meanigful about our methods we shall look at the distribution of the scores in order to make a better analysis of performance.\

\newpage
##### Distributions of scores

##### Accuracy
```{r echo=FALSE, message=FALSE, warning=FALSE}
p<-ggplot(scores, aes(x=Method, y=Acc, color=Method)) +
  geom_boxplot()+theme_minimal()+labs(x="",y="")+ggtitle("Mean accuracy among different imputations")
p
```

Taking a brief look at the distributions of accuracy score it seems quite unclear which algorithm performs the best. All medians seem to be aproximately on the same level and also first and third quartile of almost all of the plots have the same value. Only missmda is a bit lower than the others, but this is too early to derive any conclusions.\

Let's check standard deviation of scores.

```{r echo=FALSE, message=FALSE,warning=FALSE}
knitr::kable(scores%>% group_by(Method) %>% summarise(acc_std=sd(Acc,na.rm=TRUE)))
```

Deviation of scores seem to be quite equal for all of the methods so it will not be an issue.

\newpage

##### F1

```{r echo=FALSE, message=FALSE, warning=FALSE}
p<-ggplot(scores, aes(x=Method, y=F1, color=Method)) +
  geom_boxplot()+theme_minimal()+labs(x="",y="")+ggtitle("Mean F1 among different imputations")
p
```

F1 scores give us much more informations. First thing that becomes apparent after looking at that plot is fact that we do have some very low values for every type of imputation. However this is simply because some of our datasetes were very small, saw neither of our algorithms were able to achieve recall above 0. Beside from that once again scores of all algorithms were quite close to each other. This is why we decide to use two different methods of comparing and classifying these algortihms, so that we will be able to choose our winner.

The same like before let's check standard deviation to ensure that our measurments will not be biased by it.

```{r echo=FALSE, message=FALSE,warning=FALSE}
knitr::kable(scores%>% group_by(Method) %>% summarise(F1_std=sd(F1,na.rm=TRUE)))
```

Once again scores are quite equal, but the raw numbers are a bit higher, purely because of the reasons that we described earlier, which was the problem of not balanced datasets.

\newpage

#### Median baseline score

As a more reliable sample of imputation assessment we use to arrange them according to the application of the scheme. Let's take the result of each imputation method on each set. We define this result as the average of all machine processing algorithms released on data collected using imputation. Choose, we sort the result from the highest and we will assign algorithms points for each place. Finally, we add points and the method of imputation which points are the most important will win our ranking.

```{r echo=FALSE, message=FALSE,warning=FALSE}

library(R.utils)
library(prob)
dane_o_imputacij <- scores
ids <- dane_o_imputacij$Dataset
x <- c(1,2,3)
algosy <- c('log_reg','xgb','rf','svm')

for (i in ids) {
  
  indeksy <- ifelse(dane_o_imputacij$Dataset==i,TRUE,FALSE)
  dane_id <-  dane_o_imputacij[indeksy,]
  expectet <- c('median','softimpute','missmda','mice','VIM','missForest')
  founded <- dane_id$Method
  insert_<- setdiff(expectet,founded)
  
  for (j in insert_){
    for(a in algosy){
  dane_o_imputacij <- rbind(dane_o_imputacij,c(1,i,j,a,0,0))
  
  }
  dane_wynik <- dane_o_imputacij
  }
}
data<- dane_wynik
data$F1 <- as.double(data$F1)
data$acc <- as.double(data$Acc)

data[is.na(data)] <- 0
data3 <- data %>% group_by(Dataset,Method) %>% summarise(mean=mean(acc,na.rm=TRUE))
order_scores <- data3 %>% group_by(Dataset) %>% mutate(good_ranks = rank(-mean, ties.method = "max"))
order_scores1 <- order_scores %>% group_by(Method) %>% summarise(wynik = sum(good_ranks))

order_scores1 <- order_scores1[order(order_scores1$wynik),]
knitr::kable(order_scores1)
```


After these comparisons we seem to have measure of algorithms perfromarnce. We cannot clearly compare by numbers, but the order which we present here may be quite useful to present how well algortihms work.


```{r echo=FALSE, message=FALSE,warning=FALSE}
data3 <- data %>% group_by(Dataset,Method) %>% summarise(mean=mean(F1,na.rm=TRUE))
order_scores <- data3 %>% group_by(Dataset) %>% mutate(good_ranks = rank(-mean, ties.method = "max"))
order_scores1 <- order_scores %>% group_by(Method) %>% summarise(wynik = sum(good_ranks))

order_scores1 <- order_scores1[order(order_scores1$wynik),]
knitr::kable(order_scores1)
```

And the same thing for F1 score. Because of the fact, that plenty of datasets we used were not balanced we decide to use F1 score as our main measure, because it is able to operate better on such datasets.

##### Best model by F1 measure

The process which we described earlier may be also quite useful to find the best classification alghoritm among tested. That process may

```{r echo=FALSE, message=FALSE,warning=FALSE}
data3 <- data %>% group_by(Dataset,Algorithm) %>% summarise(mean=mean(F1,na.rm=TRUE))
order_scores <- data3 %>% group_by(Dataset) %>% mutate(good_ranks = rank(-mean, ties.method = "max"))
order_scores1 <- order_scores %>% group_by(Algorithm) %>% summarise(wynik = sum(good_ranks))

order_scores1 <- order_scores1[order(order_scores1$wynik),]
knitr::kable(order_scores1[1:4,])
```

Results which we gained are quite interesing, but we are afraid that some scores may be biased because of the fact that we had some very small datasets available in training which may have forced some algorithms to overfit. Nevertheless SVM scored the best with a reasonable margin above the others.

\newpage

#### Second approach

As the second form of ranking imputation algorithms, we decided to use the following formula. We treat the median as the basic form of imputation and we will compare it to all other methods. We want to check how much the average prediction measured made by other algorithms differed from the median. As one prediction, we understand the average of all imputation methods


```{r echo=FALSE, message=FALSE,warning=FALSE}
data5 <- data %>% group_by(Dataset,Method) %>% summarise(mean=mean(F1,na.rm=TRUE))
un <- length(unique(data5$Dataset))
data5[data5$Method=='median','method'] = 'aaamedian'
order_scores2 <- data5 %>% group_by(Dataset) %>% mutate(diff = max(mean) - mean)
order_scores2 <- data5 %>% group_by(Dataset) %>% mutate(diff = mean - dplyr::lag(mean, default = mean[1]))
scores <- order_scores2 %>% group_by(Method) %>% summarise(score = sum(diff)/un)
x <- scores[scores$Method!='aaamedian',]
x <- scores[x$Method!='median',]
knitr::kable(x[order(x$score,decreasing = TRUE),])

```

Result are quite simmilar to what we have achieved earlier, but here we can capture the difference which some algorithms have compared with median imputing. Difference can be even up to 0.1 in F1 score which is an considerable amount.


### Summary and conclusions 

Considering metrics scores achieved by evaluated model few important conlusions about imputation methods can be made. Basic approach with median and mode imputation allows to get decent results of machine learning model. Its disadvatage is large discrepancy in scores. More complex techniques available in packages, ensure more stable results of model among all data sets. From them, we can distinguish missFOREST, VIM irmi and mice. These packages have carried out the best imputation in terms of further model performance. They proved to be the most versatile, achiving the best average results of accuracy. VIM and missFOREST AUC scores show also the stability of model performance on imputed data set. SoftImpute package caused too low scores of recall and precision which suggests its poor performance on unbalanced data. MissMDA package performed worse than basic mode/median and all other methods, being able to proceed only on 6 from 11 data sets.  
Second aspect to consider is time needed for imputation. MissFOREST achieved good metrics scores of model, but its very time consuming. Both VIM irmi and mice packages not only provided well imputed data sets but also were time efficient.  
Choosing the best imputation tool may obviously depend on characteristic of the specific data sets, however our research should help to find flexible solution for everyday usage.

### Bibliography